<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-08-23">

<title>WildTrax: a platform for the management, storage, processing, sharing and discovery of avian data</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="wildtrax-ace-eco_files/libs/clipboard/clipboard.min.js"></script>
<script src="wildtrax-ace-eco_files/libs/quarto-html/quarto.js"></script>
<script src="wildtrax-ace-eco_files/libs/quarto-html/popper.min.js"></script>
<script src="wildtrax-ace-eco_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="wildtrax-ace-eco_files/libs/quarto-html/anchor.min.js"></script>
<link href="wildtrax-ace-eco_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="wildtrax-ace-eco_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="wildtrax-ace-eco_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="wildtrax-ace-eco_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="wildtrax-ace-eco_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="wildtrax-ace-eco_files/libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="wildtrax-ace-eco_files/libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="wildtrax-ace-eco_files/libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<link href="wildtrax-ace-eco_files/libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet">
<script src="wildtrax-ace-eco_files/libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<link href="wildtrax-ace-eco_files/libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet">
<script src="wildtrax-ace-eco_files/libs/datatables-binding-0.33/datatables.js"></script>
<script src="wildtrax-ace-eco_files/libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="wildtrax-ace-eco_files/libs/dt-core-1.13.6/css/jquery.dataTables.min.css" rel="stylesheet">
<link href="wildtrax-ace-eco_files/libs/dt-core-1.13.6/css/jquery.dataTables.extra.css" rel="stylesheet">
<script src="wildtrax-ace-eco_files/libs/dt-core-1.13.6/js/jquery.dataTables.min.js"></script>
<link href="wildtrax-ace-eco_files/libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet">
<script src="wildtrax-ace-eco_files/libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>


</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract">Abstract</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#methods" id="toc-methods" class="nav-link" data-scroll-target="#methods">Methods</a>
  <ul class="collapse">
  <li><a href="#database" id="toc-database" class="nav-link" data-scroll-target="#database">Database</a>
  <ul class="collapse">
  <li><a href="#infrastructure" id="toc-infrastructure" class="nav-link" data-scroll-target="#infrastructure">Infrastructure</a></li>
  <li><a href="#role-based-access-and-data-security" id="toc-role-based-access-and-data-security" class="nav-link" data-scroll-target="#role-based-access-and-data-security">Role-based access and data security</a></li>
  <li><a href="#schemas" id="toc-schemas" class="nav-link" data-scroll-target="#schemas">Schemas</a></li>
  </ul></li>
  <li><a href="#user-interface-and-user-experience-uiux" id="toc-user-interface-and-user-experience-uiux" class="nav-link" data-scroll-target="#user-interface-and-user-experience-uiux">User interface and user experience (UI/UX)</a>
  <ul class="collapse">
  <li><a href="#acoustic-sensor" id="toc-acoustic-sensor" class="nav-link" data-scroll-target="#acoustic-sensor">Acoustic sensor</a></li>
  <li><a href="#camera-sensor" id="toc-camera-sensor" class="nav-link" data-scroll-target="#camera-sensor">Camera sensor</a></li>
  <li><a href="#point-count-sensor" id="toc-point-count-sensor" class="nav-link" data-scroll-target="#point-count-sensor">Point count sensor</a></li>
  </ul></li>
  <li><a href="#data-publication-and-sharing" id="toc-data-publication-and-sharing" class="nav-link" data-scroll-target="#data-publication-and-sharing">Data publication and sharing</a>
  <ul class="collapse">
  <li><a href="#reports" id="toc-reports" class="nav-link" data-scroll-target="#reports">Reports</a></li>
  <li><a href="#data-discover" id="toc-data-discover" class="nav-link" data-scroll-target="#data-discover">Data Discover</a></li>
  </ul></li>
  <li><a href="#analysis-with-wildrtrax" id="toc-analysis-with-wildrtrax" class="nav-link" data-scroll-target="#analysis-with-wildrtrax">Analysis with wildrtrax</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  <li><a href="#discussion-and-future-prospects" id="toc-discussion-and-future-prospects" class="nav-link" data-scroll-target="#discussion-and-future-prospects">Discussion and future prospects</a></li>
  <li><a href="#glossary" id="toc-glossary" class="nav-link" data-scroll-target="#glossary">Glossary</a></li>
  <li><a href="#additional-information-and-declarations" id="toc-additional-information-and-declarations" class="nav-link" data-scroll-target="#additional-information-and-declarations">Additional Information and Declarations</a>
  <ul class="collapse">
  <li><a href="#competing-interests" id="toc-competing-interests" class="nav-link" data-scroll-target="#competing-interests">Competing Interests</a></li>
  <li><a href="#author-contributions" id="toc-author-contributions" class="nav-link" data-scroll-target="#author-contributions">Author Contributions</a></li>
  </ul></li>
  <li><a href="#acknowledgements" id="toc-acknowledgements" class="nav-link" data-scroll-target="#acknowledgements">Acknowledgements</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">WildTrax: a platform for the management, storage, processing, sharing and discovery of avian data</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-contents">
             <p>Alexander G. MacPhail <a href="mailto:alex.macphail@ualberta.ca" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
             <p>Corrina Copp </p>
             <p>Erin M. Bayne </p>
             <p>Michael Packer </p>
             <p>Chad Klassen </p>
             <p>Joan Fang </p>
             <p>Hedwig E. Lankau </p>
             <p>Monica Kohler </p>
             <p>Tara Narwani </p>
             <p>Steven L. Van Wilgenburg </p>
             <p>Elly C. Knight </p>
             <p>Kevin G. Kelly </p>
             <p>Charles M. Francis </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 23, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="abstract" class="level1">
<h1>Abstract</h1>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>This is a draft manuscript for submission to Avian Conservation and Ecology Special Feature: A synthesis of data, tools, and resources for avian research and conservation planning in Canada (https://ace-eco.org/feature/8/). The text and content may change as results are finalized.</p>
</div>
</div>
</div>
<p><strong>-French-</strong></p>
<p>Au fur et à mesure que les capteurs environnementaux deviennent des outils indispensables pour surveiller et évaluer les oiseaux, leur efficacité repose sur des systèmes capables de gérer les ensembles de données volumineux et souvent complexes qu’ils génèrent. WildTrax (https://www.wildtrax.ca ) est une plateforme web conçue pour gérer, stocker, traiter, partager et découvrir ces données à différentes échelles. Grâce à ses améliorations et à son entretien continus, WildTrax permet aux chercheurs de répondre à des questions écologiques sur des échelles spatiales et temporelles, tout en renforçant le réseau de chercheurs, en favorisant la collaboration et en améliorant le partage de l’information pour soutenir la conservation des oiseaux au Canada et ailleurs.</p>
<p><strong>-English-</strong></p>
<p>As environmental sensors become indispensable tools for monitoring and assessing birds, their effectiveness depends on robust systems capable of managing the large, and often overwhelming, datasets they generate. WildTrax (https://www.wildtrax.ca) is a web-based platform built to manage, store, process, share, and discover environmental sensor data from local to international scales. Through its ongoing development and maintenance, WildTrax enables researchers to address ecological questions across multiple spatial and temporal scales, while strengthening the avian data network, fostering collaboration, and enhancing data sharing to support bird conservation in Canada and beyond.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Highlights</strong></p>
<ul>
<li>Environmental sensor data generate large and complex datasets, requiring structured systems for harmonization, quality control, and integration across studies.</li>
<li>WildTrax provides a framework for storing, managing, processing, sharing, and discovering ecological data, enabling broader accessibility and reproducibility.</li>
<li>Analytical pipelines, including thoughtfully applied artificial intelligence tools, can enhance data interpretation while maintaining transparency and user control.</li>
<li>Continued development of integrated data platforms fosters opportunities for researchers to synthesize diverse datasets, advancing both ecological understanding and conservation practice.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Keywords:</strong><br>
Environmental sensors, bird population monitoring, data management, online platform, ecological data sharing, multi-scale analysis, conservation technology</p>
</div>
</div>
</div>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Birds have long been recognized as ecological indicators (<span class="citation" data-cites="temple1989bird">Temple and Wiens (<a href="#ref-temple1989bird" role="doc-biblioref">1989</a>)</span>, <span class="citation" data-cites="bibby1999making">Bibby (<a href="#ref-bibby1999making" role="doc-biblioref">1999</a>)</span>, <span class="citation" data-cites="canterbury2000bird">Canterbury et al. (<a href="#ref-canterbury2000bird" role="doc-biblioref">2000</a>)</span>, <span class="citation" data-cites="gregory2003using">Gregory et al. (<a href="#ref-gregory2003using" role="doc-biblioref">2003</a>)</span>, <span class="citation" data-cites="mekonen2017birds">Mekonen (<a href="#ref-mekonen2017birds" role="doc-biblioref">2017</a>)</span>) due to their sensitivity to environmental changes (<span class="citation" data-cites="furness2013birds">Furness and Greenwood (<a href="#ref-furness2013birds" role="doc-biblioref">2013</a>)</span>), broad distribution across ecosystems (<span class="citation" data-cites="orme2006global">Orme et al. (<a href="#ref-orme2006global" role="doc-biblioref">2006</a>)</span>, <span class="citation" data-cites="jetz2012global">Jetz et al. (<a href="#ref-jetz2012global" role="doc-biblioref">2012</a>)</span>, <span class="citation" data-cites="greenberg2012timelapse">Greenberg and Godin (<a href="#ref-greenberg2012timelapse" role="doc-biblioref">2012</a>)</span>, <span class="citation" data-cites="aide2013real">Aide et al. (<a href="#ref-aide2013real" role="doc-biblioref">2013</a>)</span>, <span class="citation" data-cites="lepage2014avibase">Lepage, Vaidya, and Guralnick (<a href="#ref-lepage2014avibase" role="doc-biblioref">2014</a>)</span>, <span class="citation" data-cites="ahumada2020wildlife">Ahumada et al. (<a href="#ref-ahumada2020wildlife" role="doc-biblioref">2020</a>)</span>), and measurable population dynamics (<span class="citation" data-cites="reif2013long">Reif (<a href="#ref-reif2013long" role="doc-biblioref">2013</a>)</span>). They can signal shifts in ecosystem health (<span class="citation" data-cites="zakaria2005comparison">Zakaria, Leong, and Yusuf (<a href="#ref-zakaria2005comparison" role="doc-biblioref">2005</a>)</span>, <span class="citation" data-cites="newman2007aquatic">Newman et al. (<a href="#ref-newman2007aquatic" role="doc-biblioref">2007</a>)</span>, <span class="citation" data-cites="smits2013avian">Smits and Fernie (<a href="#ref-smits2013avian" role="doc-biblioref">2013</a>)</span>) and function (<span class="citation" data-cites="sekercioglu2012bird">Sekercioglu (<a href="#ref-sekercioglu2012bird" role="doc-biblioref">2012</a>)</span>, <span class="citation" data-cites="sitters2016bird">Sitters et al. (<a href="#ref-sitters2016bird" role="doc-biblioref">2016</a>)</span>, <span class="citation" data-cites="morante2017appraisal">Morante-Filho and Faria (<a href="#ref-morante2017appraisal" role="doc-biblioref">2017</a>)</span>), providing integral metrics (<span class="citation" data-cites="stanton2016flexible">Stanton et al. (<a href="#ref-stanton2016flexible" role="doc-biblioref">2016</a>)</span>, <span class="citation" data-cites="michel2020metrics">Michel et al. (<a href="#ref-michel2020metrics" role="doc-biblioref">2020</a>)</span>) for conservation and management. Birds are also valuable indicators because they occupy diverse niches, have well-studied life histories, and often correlate with the health of other taxa (<span class="citation" data-cites="fleishman2005using">Fleishman et al. (<a href="#ref-fleishman2005using" role="doc-biblioref">2005</a>)</span>). Their utility spans ecosystem monitoring, habitat quality assessment, and gauging the impact of environmental stressors such as land use change, climate change (<span class="citation" data-cites="jiguet2007climate">Jiguet et al. (<a href="#ref-jiguet2007climate" role="doc-biblioref">2007</a>)</span>), and pollution (<span class="citation" data-cites="niemi1997critical">Niemi et al. (<a href="#ref-niemi1997critical" role="doc-biblioref">1997</a>)</span>, <span class="citation" data-cites="mekonen2017birds">Mekonen (<a href="#ref-mekonen2017birds" role="doc-biblioref">2017</a>)</span>). Advances in statistical modelling now allow for better integration of uncertainty, phylogenetic relationships, and temporal autocorrelation, enhancing the reliability of bird-based indicators (<span class="citation" data-cites="fraixedas2020state">Fraixedas et al. (<a href="#ref-fraixedas2020state" role="doc-biblioref">2020</a>)</span>). However, there remains challenges in their application as indicators, including spatial, seasonal, and habitat biases, as well as insufficient consideration of statistical uncertainty and temporal autocorrelation in multi-species bird indicators (<span class="citation" data-cites="gregory2003using">Gregory et al. (<a href="#ref-gregory2003using" role="doc-biblioref">2003</a>)</span>, <span class="citation" data-cites="fraixedas2020state">Fraixedas et al. (<a href="#ref-fraixedas2020state" role="doc-biblioref">2020</a>)</span>).</p>
<p>Environmental sensors, such as autonomous recording units (ARUs) and remote camera traps, are reshaping avian monitoring by providing continuous, high-resolution, and large-scale data collection (<span class="citation" data-cites="hobson2002acoustic">Hobson et al. (<a href="#ref-hobson2002acoustic" role="doc-biblioref">2002</a>)</span>, <span class="citation" data-cites="shonfield2017autonomous">Shonfield and Bayne (<a href="#ref-shonfield2017autonomous" role="doc-biblioref">2017</a>)</span>, <span class="citation" data-cites="pollet2025technological">Pollet et al. (<a href="#ref-pollet2025technological" role="doc-biblioref">2025</a>)</span>). The adoption of ARUs has allowed single-visit human surveys to be supplemented or replaced with archived acoustic recordings, enabling “big data” approaches that integrate multiple datasets for broader ecological inference (<span class="citation" data-cites="hampton2013big">Hampton et al. (<a href="#ref-hampton2013big" role="doc-biblioref">2013</a>)</span>; <span class="citation" data-cites="farley2018situating">Farley et al. (<a href="#ref-farley2018situating" role="doc-biblioref">2018</a>)</span>; <span class="citation" data-cites="shin2015ecological">Shin and Choi (<a href="#ref-shin2015ecological" role="doc-biblioref">2015</a>)</span>; <span class="citation" data-cites="nathan2022big">Nathan et al. (<a href="#ref-nathan2022big" role="doc-biblioref">2022</a>)</span>; <span class="citation" data-cites="peters2014harnessing">Peters et al. (<a href="#ref-peters2014harnessing" role="doc-biblioref">2014</a>)</span>; <span class="citation" data-cites="hallgren2016biodiversity">Hallgren et al. (<a href="#ref-hallgren2016biodiversity" role="doc-biblioref">2016</a>)</span>) while still being statistical harmonized across methodologies (<span class="citation" data-cites="solymos2013calibrating">Sólymos et al. (<a href="#ref-solymos2013calibrating" role="doc-biblioref">2013</a>)</span>) with detectability rates depend on factors such as distance from the ARU, frequency range, and habitat structure (<span class="citation" data-cites="yip2017experimentally">Yip et al. (<a href="#ref-yip2017experimentally" role="doc-biblioref">2017</a>)</span>). Remote camera traps have likewise become valuable tools for avian research, particularly for documenting nesting behavior, predation events, and species presence in otherwise inaccessible habitats (<span class="citation" data-cites="bolton2007remote">Bolton et al. (<a href="#ref-bolton2007remote" role="doc-biblioref">2007</a>)</span>; <span class="citation" data-cites="randler2018distance">Randler and Kalb (<a href="#ref-randler2018distance" role="doc-biblioref">2018</a>)</span>; <span class="citation" data-cites="o2008picture">O’Brien and Kinnaird (<a href="#ref-o2008picture" role="doc-biblioref">2008</a>)</span>). When deployed alongside ARUs, cameras provide complementary visual evidence that strengthens ecological inference, highlighting the advantages of multimodal sensor programs (<span class="citation" data-cites="buxton2018pairing">Buxton et al. (<a href="#ref-buxton2018pairing" role="doc-biblioref">2018</a>)</span>; <span class="citation" data-cites="garland2020acoustic">Garland et al. (<a href="#ref-garland2020acoustic" role="doc-biblioref">2020</a>)</span>). Both sensors generate large ecological media datasets, often characterized as ‘big data’ due to their volume, variety (e.g., different file types), veracity (uncertainty in data quality such as noise or misclassification), and velocity (rate of accumulation or acquistion, <span class="citation" data-cites="hampton2013big">Hampton et al. (<a href="#ref-hampton2013big" role="doc-biblioref">2013</a>)</span>; <span class="citation" data-cites="farley2018situating">Farley et al. (<a href="#ref-farley2018situating" role="doc-biblioref">2018</a>)</span>). Centralized software platforms facilitate standardization, integration, and sharing of these data, enabling interdisciplinary collaboration and advancing biodiversity research, and helping to reduce bias (<span class="citation" data-cites="peters2014harnessing">Peters et al. (<a href="#ref-peters2014harnessing" role="doc-biblioref">2014</a>)</span>). Notably, machine learning is being combined with ecological models to meet computational demands, positioning sensors and big data systems to link raw ecological observations with actionable conservation strategies and sustainability goals (<span class="citation" data-cites="nathan2022big">Nathan et al. (<a href="#ref-nathan2022big" role="doc-biblioref">2022</a>)</span>) and minimizing data waste (<span class="citation" data-cites="binley2023minimizing">Binley et al. (<a href="#ref-binley2023minimizing" role="doc-biblioref">2023</a>)</span>).</p>
<p>To maximize their utility in an open framework, biodiversity data must align with FAIR principles: findable, accessible, interoperable, and reusable (<span class="citation" data-cites="kush2020fair">Kush et al. (<a href="#ref-kush2020fair" role="doc-biblioref">2020</a>)</span>). However, challenges persist around data quality, equitable access, and long-term preservation, highlighting the importance of strong and lasting socio-technical frameworks (<span class="citation" data-cites="shin2015ecological">Shin and Choi (<a href="#ref-shin2015ecological" role="doc-biblioref">2015</a>)</span>). Software platforms that implement these principles and harmonize diverse datasets while maintaining consistency, equity, and quality can unlock advanced species- and community-level analyses, ultimately transforming raw observations into actionable insights for conservation and policy (<span class="citation" data-cites="stephenson2020inventory">Stephenson and Stengel (<a href="#ref-stephenson2020inventory" role="doc-biblioref">2020</a>)</span>; <span class="citation" data-cites="fox2017generating">Fox et al. (<a href="#ref-fox2017generating" role="doc-biblioref">2017</a>)</span>). By overcoming limitations of traditional methods and promoting open data-sharing, such platforms enhance understanding of ecological trends (<span class="citation" data-cites="buxton2021key">Buxton et al. (<a href="#ref-buxton2021key" role="doc-biblioref">2021</a>)</span>) and foster collaboration among stakeholders, aligning priorities across regional to global scales (<span class="citation" data-cites="kartez2008information">Kartez and Casto (<a href="#ref-kartez2008information" role="doc-biblioref">2008</a>)</span>). Here we introduce WildTrax, a platform for storing, managing, processing, sharing, and discovering avian environmental sensor data. This paper describes version 2.0 of WildTrax, corresponding with its 2025 release.</p>
</section>
<section id="methods" class="level1 page-columns page-full">
<h1>Methods</h1>
<section id="database" class="level2">
<h2 class="anchored" data-anchor-id="database">Database</h2>
<section id="infrastructure" class="level3">
<h3 class="anchored" data-anchor-id="infrastructure">Infrastructure</h3>
<p>WildTrax uses PostgreSQL, a free and open-source relational database management system (RDBMS). PostgreSQL is well-suited for managing complex biodiversity datasets and rich metadata that accompanies ARU and remote camera data, offering optimized querying, relational structure, and robust storage capabilities (<span class="citation" data-cites="douglas2003postgresql">Douglas and Douglas (<a href="#ref-douglas2003postgresql" role="doc-biblioref">2003</a>)</span>; <span class="citation" data-cites="zhang2009efficiently">Zhang, Gertz, and Gruenwald (<a href="#ref-zhang2009efficiently" role="doc-biblioref">2009</a>)</span>; <span class="citation" data-cites="kim2021ecobank">Kim et al. (<a href="#ref-kim2021ecobank" role="doc-biblioref">2021</a>)</span>). The application infrastructure runs on a virtualized server environment configured for dedicated applications. This architecture supports both daily user interactions and more computationally intensive batch tasks, such as uploading large volumes of audio and image files or running artificial intelligence classifiers for species recognition or object detection. To ensure reliability and performance at scale, the system employs load balancing and redundancy measures, enabling near-continuous availability for a large and growing user base. The production server is hosted at the University of Alberta (Edmonton, Canada), with optional long-term storage services available both at the University of Alberta server or through Amazon Web Services (AWS) with nodes available in Calgary, Alberta, Canada; Montreal, Quebec, Canada; and Oregon, USA. Off-site storage uses <a href="https://aws.amazon.com/s3/storage-classes/glacier/">AWS Deep Glacier</a> providing a recommended 3-2-1 data backup policy (<span class="citation" data-cites="perkel2019ways">Perkel (<a href="#ref-perkel2019ways" role="doc-biblioref">2019</a>)</span>). To optimize performance, a local M.2 drive paired with a CPU significantly reduces I/O bottlenecks, enabling data read / write speeds multiple times faster than traditional SSD NAS configurations. In practice, these enhancements improve query response times and batch processing speeds, while the upgraded CPU delivers smoother responsiveness and improved multitasking during high-demand operations. Media files are indexed and linked to the storage location option an Organization chooses.</p>
</section>
<section id="role-based-access-and-data-security" class="level3">
<h3 class="anchored" data-anchor-id="role-based-access-and-data-security">Role-based access and data security</h3>
<p>WildTrax uses <a href="https://auth0.com/">Auth0</a>, a third-party role-based access control (RBAC) system that provides secure login and token-based authentication and authorization. Data membership and access follow the principle of least privilege, with permissions managed at both the Organization and Project levels. At the Organization level, users are assigned either an Administrator role, with full read–write access, or a Read-Only role, which restricts modifications to protect data integrity. Organizations may also designate a Principal Investigator (PI), who serves as the primary account for handling access requests. If a PI is not assigned, requests are directed to Organization and Project Administrators. At the Project level, roles provide more granular control. Administrators have full management access, including user assignments and data syncing. Taggers can annotate media but do not have administrative privileges. Read-Only users have view-only access, enabling collaboration without altering project data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="assets/user-assignments.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="User assignments"><img src="assets/user-assignments.png" class="img-fluid figure-img" alt="User assignments"></a></p>
<figcaption>User assignments</figcaption>
</figure>
</div>
</section>
<section id="schemas" class="level3">
<h3 class="anchored" data-anchor-id="schemas">Schemas</h3>
<p>WildTrax employs a hierarchical structure to maintain data integrity, enforce standardization, and support interoperability of environmental sensors. The system is built around three primary data schemas, ARUs, cameras, and point counts, which are organized within Projects. Projects serve as centralized containers for all observations and sensor outputs linked to a specific study or research question. At a higher level, Organizations aggregate multiple Projects, bringing sensor data and media together under a unified framework. In addition, species schemas are managed across multiple taxonomic levels, enabling both standardized presets and the custom addition of species within individual Projects.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="assets/common-schema.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Entity relationship diagram of the common WildTrax schema"><img src="assets/common-schema.png" class="img-fluid figure-img" alt="Entity relationship diagram of the common WildTrax schema"></a></p>
<figcaption>Entity relationship diagram of the common WildTrax schema</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="assets/organization-locations.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Organization locations tab"><img src="assets/organization-locations.png" class="img-fluid figure-img" alt="Organization locations tab"></a></p>
<figcaption>Organization locations tab</figcaption>
</figure>
</div>
<p>Media uploaded to WildTrax, whether recordings or images, must include a string name prefix and media date-time stamp. For recordings this is contained in the media file name (e.g.&nbsp;<code>LOCATION_YYYYMMDD_HHMMSS</code>) and within the parent folder name and EXIF metadata in image files. This way, each media file is explicitly linked to its parent Location, the precise geographic point of deployment, ensuring that sensors and their outputs remain inseparable from their environmental context. The schema enforces referential integrity while streamlining front-end operations such as data retrieval, filtering, and visualization through optimized queries and external APIs. For example, temporal metadata is standardized across sensors (<code>YYYY-MM-DD HH:MM:SS</code>), while spatial coordinates are consistently defined at the Location level (longitude, latitude; WGS84). This framework enables multi-dimensional analyses that integrate raw sensor output with ecological, climatic, and biogeographic patterns. From the parent Location, metadata can be expanded through various Organizational tabs: Location Photos for visual records of the landscape or context of the Location, Visits for records of when an observer visited a Location; Equipment, an indexed inventory of devices used to collect media; and Deployments, the record of a specific piece of equipment deployed during a Visit. Together, these metadata provide users with precise information on the exact equipment used to survey each Location and collect media files. This detail strengthens data quality by enabling issues related to mechanical failure, physical damage, environmental conditions, or recording errors to be traced and validated later during processing tasks.</p>
<p float="left">
<img src="assets/pc-projects.png" alt="Point count project dashboard" width="45%"> <img src="assets/aru-projects.png" alt="ARU project dashboard" width="45%">
</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="assets/aru-schema.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Entity relationship diagram of the acoustic (ARU) WildTrax schema"><img src="assets/aru-schema.png" class="img-fluid figure-img" alt="Entity relationship diagram of the acoustic (ARU) WildTrax schema"></a></p>
<figcaption>Entity relationship diagram of the acoustic (ARU) WildTrax schema</figcaption>
</figure>
</div>
</section>
</section>
<section id="user-interface-and-user-experience-uiux" class="level2">
<h2 class="anchored" data-anchor-id="user-interface-and-user-experience-uiux">User interface and user experience (UI/UX)</h2>
<p>WildTrax’s user interface is a responsive web application built with <a href="https://vuejs.org/,%20version%203">Vuejs</a>, leveraging its modular component-based architecture, including the Composition API, for scalability, maintainability, and efficient logic reuse. The user interface (UI) is styled and enhanced using <a href="https://primevue.org/">PrimeVue</a>, among other libraries, which delivers rich, customization components such as data tables and dashboards for seamless data exploration and visualization. The application is deployed via an Apache HTTP Server, which serves the front-end and handles API routing through reverse-proxy configurations to back-end services. WildTrax exposes APIs for data exchange and provides export tools in standard scientific formats (e.g., CSV, JSON, text and zip), supporting downstream integration with statistical and geospatial workflows.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="assets/context-menu-public.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="ARU context menu for a public project"><img src="assets/context-menu-public.png" class="img-fluid figure-img" alt="ARU context menu for a public project"></a></p>
<figcaption>ARU context menu for a public project</figcaption>
</figure>
</div>
<p>Dashboards are designed to give users a clear, intuitive overview of their sensors’ data, enabling both novice and expert users to navigate and interpret project information with minimal friction. Key features, including filters, sorting, clearly labeled column headers with hover information, and tooltips, are complemented by dropdown menus accessed through responsive, context-aware controls, allowing users to refine or manipulate large datasets quickly without navigating complex database relationship structures. Visual elements such as sortable data tables, progress indicators, and status icons support efficient user scanning and pattern recognition, while maintaining consistency with the broader WildTrax design system. Attention to micro-interactions, such as inline feedback and notifications when data are updated or filters applied, reinforces a sense of responsiveness and reduces cognitive user load. The dashboard prioritizes transparency by linking each dataset directly back to its associated location and media, ensuring users can trace results from summary views down to the raw sensor files from Tasks to Organizations. WildTrax 2.0 incorporates a new content management system (CMS) that provides support for translation (currently supported in English and French), internationalization (i18n), and content localization, ensuring that the platform can accommodate multilingual users and diverse regional requirements.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="assets/cms.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Content management system"><img src="assets/cms.png" class="img-fluid figure-img" alt="Content management system"></a></p>
<figcaption>Content management system</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="assets/aru-upload.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="ARU recording upload popup"><img src="assets/aru-upload.png" class="img-fluid figure-img" alt="ARU recording upload popup"></a></p>
<figcaption>ARU recording upload popup</figcaption>
</figure>
</div>
<p>Each sensor processing interface and data management capabilities are tailored to the unique characteristics of its respective data type while ensuring consistency in data harmonization, design, and usability. Within each sensor type, ARUs, cameras and point counts, Projects serve as the core organizational unit, enabling users to upload media, manage processing tasks from recordings or image sets, integrate external data, assign species and user roles, verify species tags, attach ancillary metadata and files, and publish results for discovery and use by other members and the broader community.</p>
<section id="acoustic-sensor" class="level3">
<h3 class="anchored" data-anchor-id="acoustic-sensor">Acoustic sensor</h3>
<p>The design of the acoustic sensor workflow is to provide a few steps as possible to maximize the use of the acoustic data for any level of user. Supported audio files types are WAC and W4V (<a href="https://www.wildlifeacoustics.com/">Wildlife Acoustics</a> proprietary lossless compressed file types), <a href="https://github.com/xiph/flac">FLAC</a> (open-source lossless compressed file types), MP3 (lossy compressed audio; (MPEG-1 Audio Layer III)) and WAV (uncompressed audio). All uploaded file types, except MP3, are converted to using a lossless audio codec, FLAC, that preserves bit-for-bit fidelity while reducing storage needs by approximately 30–70% compared to WAV. FLAC also allows researchers to uncompress the lossless data for use in other applications, avoided any issues that arise with lossy data compression (<span class="citation" data-cites="macphail2024audio">MacPhail et al. (<a href="#ref-macphail2024audio" role="doc-biblioref">2024</a>)</span>).</p>
<p>Recordings can be uploaded to either Projects or Organizations, with all media ultimately owned and stored at the Organization level. Within an Organization, recordings are aggregated in the Recordings tab, where users can generate Tasks based on pre-selected criteria with over 40 filters are available, grouped into Weather, Location, Recording, Species, and Astronomical conditions. For example, users can select recordings with a BirdNET confidence threshold of 0.8, during a new moon, and between 6 AM and 9 AM. These selected recordings are placed in a cart and can then be generated into a Project. This workflow is unprecedented, as it allows users to manage all their media centrally while creating Projects only for the specific questions they want to investigate.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="assets/task-selector.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Acoustic task selection process with filters applied for recordings with Swainson’s Thrushes on partly cloudly detected"><img src="assets/task-selector.png" class="img-fluid figure-img" alt="Acoustic task selection process with filters applied for recordings with Swainson’s Thrushes on partly cloudly detected"></a></p>
<figcaption>Acoustic task selection process with filters applied for recordings with Swainson’s Thrushes on partly cloudly detected</figcaption>
</figure>
</div>
<p>At the Project level, uploaded recordings become Tasks also allowing the users to assign a duration, processing method and observer to their recording. This unique combination allow recordings to be re-processed at different durations, observers or processing methods allowing for multi-observer data processing (<span class="citation" data-cites="macphail2025unpublished">(<a href="#ref-macphail2025unpublished" role="doc-biblioref"><strong>macphail2025unpublished?</strong></a>)</span>) and flexibililty of re-using the same audio data for different processing methodologies or questions. Once audio recordings are archived, they are then dynamically converted into spectrograms, visual representations of the audio signals using short-time Fourier transforms (STFT) using the <a href="https://sourceforge.net/projects/sox/">SoX</a> within the acoustic processing interface. Default project parameters (X-scale for duration of time and Y-scale for spectrogram height in pixels, and colour formatting) define the size and style of the spectrogram but can be changed dynamically using the Audio Settings panel to modify the dimensions and range of the spectral signatures in order to isolate frequency ranges. This is particularly useful to isolate the range of species that can in narrow frequency bands.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/dynamic-audio-settings.png" class="ligthbox img-fluid figure-img"></p>
<figcaption>Dynamic audio settings panel</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="assets/ruffed-grouse.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Ruffed Grouse non-vocal drumming"><img src="assets/ruffed-grouse.png" class="img-fluid figure-img" alt="Ruffed Grouse non-vocal drumming"></a></p>
<figcaption>Ruffed Grouse non-vocal drumming</figcaption>
</figure>
</div>
<p>The processing interface allows users to play back audio while simultaneously viewing the spectrogram, providing both auditory and visual interaction with the data. This setup enables users to isolate individuals across recordings by examining signal directionality (left vs.&nbsp;right channel), amplitude (e.g.&nbsp;intensity or loudness), timing or frequency of vocalizations, and distinct song types. By combining these features with visual and auditory interpretation, experts can curate recordings with high precision and accuracy for both species identification and count estimates.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="assets/yellow-warblers.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Acoustic processing interface showcasing multiple Yellow Warblers signing with different intensities, directionality and timing"><img src="assets/yellow-warblers.png" class="img-fluid figure-img" alt="Acoustic processing interface showcasing multiple Yellow Warblers signing with different intensities, directionality and timing"></a></p>
<figcaption>Acoustic processing interface showcasing multiple Yellow Warblers signing with different intensities, directionality and timing</figcaption>
</figure>
</div>
<p>Various panels support and enhance the acoustic processing UX and accessibility to external data. The Weather panel provides environmental context, including temperature, wind speed, precipitation at the nearest hour of the recording, as well as sunrise, sunset, moonrise, moonset, and lunar phase. The AI-assisted classification panel displays species predictions at 1.5-second intervals along the spectrogram, with outputs filterable by classifier-specific score thresholds (BirdNET and HawkEars), with access to the overlays enabled by Project Administrators. The Noise panel allows users to annotate geophonic noise (e.g., wind, rain, ocean), anthropogenic noise (e.g., industry, traffic), or equipment malfunctions. Noise events can be further categorized by channel (left/right), amplitude (low to extreme), and frequency (intermittent, frequent, constant, infrequent). Access to Location Photos also aids species identification and helps assess landscape factors, such as vegetation density, forest type or human features, that may influence species detection or noise parameters.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="assets/acoustic-sensor.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Acoustic sensor layout with major components"><img src="assets/acoustic-sensor.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="float:left; border:1px solid #ccc; padding:4px;" alt="Acoustic processing interface"></a></p>
</figure>
</div>
<figcaption>Acoustic sensor layout with major components</figcaption>
</figure>
</div>
<p>Species detections are recorded as Tags, created when a human observer draws a bounding box around an acoustic signal on the spectrogram. Each Tag captures temporal and frequency information, including date and time of first detection, duration, and frequency range (Hz). The Tag Info Panel enables rapid entry of metadata such as individual identifiers, count estimates, vocalization type, confidence flags, and comments. This structure allows flexibility, for example, a flocking species may be represented by a single Tag with a count estimate greater than one. Tagging is further guided by the Task Method, which enforces consistency and reduces inappropriate annotations.</p>
<p>Automated classification outputs (e.g., single-species recognizers or alternative models) can follow a parallel workflow. Recordings are uploaded and Tasks created, after which classifier-generated Tags are imported via the sync functionality, which supports CSV-based import and export. Users can then be assigned as validators to review, correct, or remove Tags and even rate them to assess classifier performance relative to score thresholds. Some classifiers may only provide limited detection information, such as a window or start time. In these cases, classifier Tags can be uploaded using just the signal start time, and WildTrax will generate the average frequency and duration of the species’ signal based on existing Tags in the database. Users can then adjust Tag dimensions manually if necessary. When recordings are selected using classifier outputs from the Task generator within an Organization, it is recommended that a human review the recording by drawing a Tag, providing a direct comparison of time-to-first-detection and score threshold between human and classifier annotations.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/species-verification.png" class="img-fluid figure-img"></p>
<figcaption>Acoustic species verification</figcaption>
</figure>
</div>
<p>As Tasks are marked Complete, Tags are aggregated by species in the Species Verification tab. Users are then assigned privileges in the Project as validators to review, confirm or edit key information (e.g., species ID, count, vocalization). Tags are first created or imported, then reviewed by species–vocalization type. Each Tag is either confirmed as Verified or reclassified as Transcribed. The Species tab provides summaries of verification status along with tools for managing validators and tracking workflow progress.</p>
</section>
<section id="camera-sensor" class="level3">
<h3 class="anchored" data-anchor-id="camera-sensor">Camera sensor</h3>
<p>The camera sensor in WildTrax is designed supports the ingestion, processing, and validation of image data from remote cameras. Images are uploaded at the Project level, with ownership and long-term storage again retained by the Organization, and role-based access accordingly, similar to ARU. Image sets are created from the start and end date of deployed images which then become Tasks, a unique combination of Project, User and Image Set, which can be assigned to analysts for processing and validation. Upon upload, Project Administrators can automatically pre-process images using MegaDetector V6 (<span class="citation" data-cites="beery2023megadetector">Beery (<a href="#ref-beery2023megadetector" role="doc-biblioref">2023</a>)</span>), a convolutional neural network that detects animals, people, vehicles, and empty images. It performs especially well on medium- to large-bodied mammals, humans, and vehicles, and effectively filters out false triggers and blanks. However, its performance is limited, particularly in cases of partial occlusion, rapid movement, or low image resolution. For this reason, human tagging still remains a required step to ensure accuracy and to add finer-scale annotations and metadata</p>
<p><a href="assets/camera-tagging.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img src="assets/camera-tagging.png" class="img-fluid"></a></p>
<p>After pre-classification, images enter the human tagging stage, where analysts validate or correct automated tags and add additional metadata. Metadata are divided into two main categories, selectable in the camera Project settings: tag-level metadata (e.g., species, sex, age class, behavior) and image-level metadata. To address the limitations of automated avian classification, WildTrax allows analysts to assign species codes to birds not detected by MegaDetector and to supplement metadata across sequences where birds are observed. Multiple individuals and behaviors can be tracked flexibly.</p>
<p><a href="assets/camera-verification.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img src="assets/camera-verification.png" class="img-fluid"></a></p>
<p>The processing interface supports two primary analysis modes. Series Mode displays images in chronological order, preserving event structure and enabling interpretation of sequences such as repeated detections or interspecific interactions. Tagging Mode provides access to all images via a filterable interface, allowing users to isolate subsets based on pre-classified tags, metadata, or project-specific codes. Critical metadata, such as camera field of view (FOV), is recorded as a boolean, indicating whether the camera maintained its expected setup during deployment—a key factor for detection radius and species detectability. As tags accumulate throughout a camera project, they are aggregated under the same Species Verification framework used for ARU data, with tag- and image-level metadata filters facilitating rapid verification.</p>
</section>
<section id="point-count-sensor" class="level3">
<h3 class="anchored" data-anchor-id="point-count-sensor">Point count sensor</h3>
<p><a href="assets/pc-dashboard.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14"><img src="assets/pc-dashboard.png" class="img-fluid"></a></p>
<p>The point count sensor functions as a central repository for the <a href="borealbirds.ca">Boreal Avian Modelling Centre’s</a>, or BAM’s, point count data, consolidating observations from multiple surveys into a standardized framework, and making them shareable and discoverable to the public. Point counts are harmonized into specific distance bands and durations, ensuring that methods are consistent with those used for Autonomous Recording Units (ARUs), which facilitates integrated analyses across the two data types that collect avian data. When exporting ARU data, users can configure it to match a point count format, for example, treating all detections within a single ARU as belonging to a 0–INF distance band, helps to allow synthesis with traditional point count datasets ensuring comparability of abundance estimates, species occurrence, and spatial patterns between ARU and human-observed point count data.</p>
<p><a href="assets/pc-surveys.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15"><img src="assets/pc-surveys.png" class="img-fluid"></a></p>
</section>
</section>
<section id="data-publication-and-sharing" class="level2">
<h2 class="anchored" data-anchor-id="data-publication-and-sharing">Data publication and sharing</h2>
<p>When the Project is completed and all Tasks processed, the status of the Project can be changed to a published status. Project publication allows other WildTrax users, who are not Project members, to access the media, metadata or species detections from the project either through the Project context menus to Download Report or <a href="https://discover.wildtrax.ca/">Data Discover</a>. The publication status will control how data will become visible across the system. Project publication will lock users from editing species detections and is considered the final version of the data. In addition, for every published Project, WildTrax facilitates the open avian data network with converted with a format flag on the <code>download-report</code> API to meet BDME (<a href="https://naturecounts.ca/nc/default/nc_bmde.jsp">Bird Monitoring Database Exchange</a>) with <a href="https://naturecounts.ca/nc/default/main.jsp">NatureCounts</a>. Only Map + Report and Public Projects are sent to NatureCounts with the media staying accessible through WildTrax.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>statuses <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">Status =</span> <span class="fu">c</span>(</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Test"</span>,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Active"</span>,</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Published – Private"</span>,</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Published – Map Only"</span>,</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Published – Map + Report Only"</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Published – Public"</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>  ),</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">Description =</span> <span class="fu">c</span>(</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"For users just getting started with an environmental sensor program or testing WildTrax functionalities. Project data are not avaialble for Download or in Data Discover except to project members."</span>,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Default status when a project is created. Active projects are currently being processed, designed, or in early stages of data uploading. Use for any general active work."</span>,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Project data are only available to project members. Other users must request access to view details such as species or locations."</span>,</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Project data are visible in Data Discover, but media and reports are not accessible to non-project members. Location buffering and visibility settings apply."</span>,</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Project data are available for Download and through Data Discover, but media is not accessible. Useful if data can be public but media must remain private. Does not exist for point counts."</span>,</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"All project data and details are publicly available for Download and through Data Discover. Location buffering and visibility settings still apply for non-members."</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="fu">datatable</span>(</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>  statuses,</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">rownames =</span> <span class="cn">FALSE</span>,</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>  <span class="at">options =</span> <span class="fu">list</span>(<span class="at">pageLength =</span> <span class="dv">10</span>, <span class="at">autoWidth =</span> <span class="cn">TRUE</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<div class="datatables html-widget html-fill-item" id="htmlwidget-daf664a6042db2ef7e62" style="width:100%;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-daf664a6042db2ef7e62">{"x":{"filter":"none","vertical":false,"data":[["Test","Active","Published – Private","Published – Map Only","Published – Map + Report Only","Published – Public"],["For users just getting started with an environmental sensor program or testing WildTrax functionalities. Project data are not avaialble for Download or in Data Discover except to project members.","Default status when a project is created. Active projects are currently being processed, designed, or in early stages of data uploading. Use for any general active work.","Project data are only available to project members. Other users must request access to view details such as species or locations.","Project data are visible in Data Discover, but media and reports are not accessible to non-project members. Location buffering and visibility settings apply.","Project data are available for Download and through Data Discover, but media is not accessible. Useful if data can be public but media must remain private. Does not exist for point counts.","All project data and details are publicly available for Download and through Data Discover. Location buffering and visibility settings still apply for non-members."]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>Status<\/th>\n      <th>Description<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"autoWidth":true,"columnDefs":[{"name":"Status","targets":0},{"name":"Description","targets":1}],"order":[],"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/ecosystem-health-nc.png" class="img-fluid figure-img"></p>
<figcaption>ABMI Ecosystem Health Project on NatureCounts found at https://naturecounts.ca/nc/default/datasets.jsp?code=WILDTRAX1&amp;sec=bmdr</figcaption>
</figure>
</div>
<section id="reports" class="level3">
<h3 class="anchored" data-anchor-id="reports">Reports</h3>
<p>Each sensor is customized with a series of reports of data. Reports are purported to be abstractions and summaries of data collected across each sensor for various reasons. Each report serves a specific purpose to allow downstream users to analyze their data.</p>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># wildrtrax report downloads for main report</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>report <span class="ot">&lt;-</span> <span class="fu">wt_download_report</span>(<span class="dv">620</span>, <span class="st">'ARU'</span>, <span class="at">reports =</span> <span class="st">'main'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(report)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "organization"                "project_id"                 
 [3] "location"                    "location_id"                
 [5] "location_buffer_m"           "longitude"                  
 [7] "latitude"                    "equipment_make"             
 [9] "equipment_model"             "recording_id"               
[11] "recording_date_time"         "task_id"                    
[13] "aru_task_status"             "task_duration"              
[15] "task_method"                 "species_code"               
[17] "species_common_name"         "species_scientific_name"    
[19] "individual_order"            "tag_id"                     
[21] "individual_count"            "vocalization"               
[23] "detection_time"              "tag_duration"               
[25] "rms_peak_dbfs"               "tag_is_verified"            
[27] "tag_rating"                  "observer"                   
[29] "observer_id"                 "species_individual_comments"
[31] "task_comments"              </code></pre>
</div>
</div>
<p>WildTrax structures outputs from each sensor into standardized reports containing the most relevant for its end user.</p>
</section>
<section id="data-discover" class="level3">
<h3 class="anchored" data-anchor-id="data-discover">Data Discover</h3>
<p>Data Discover is the central hub for exploring environmental sensor data in WildTrax. In Data Discover, users and the public can search for data from ARUs, cameras, and point counts, using a variety of attribute filters, and create summary statistics within a dynamic mapping interface. Users can gain a comprehensive understanding of environmental sensor data in an area that interests them noting which organizations have published data on WildTrax, which species were detected and to what frequency, and explore media elements such as images and sounds captured in the environment.</p>
<p><a href="assets/data-discover1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16"><img src="assets/data-discover1.png" class="img-fluid"></a></p>
<div class="cell">
<div class="cell-output-display">
<div class="datatables html-widget html-fill-item" id="htmlwidget-c95786e686120debdf6a" style="width:100%;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-c95786e686120debdf6a">{"x":{"filter":"none","vertical":false,"caption":"<caption>Sensor reports available in WildTrax<\/caption>","data":[["Project","Location","Recordings","Tags","Main","AI","MegaDetector","Image set"],["Descriptive Project information from the Methods and Results Summary","Locations specific to the Project, their coordinates, and visibility metadata","Recordings and their metadata","Annotations generated by human observers or classifiers","High-level summary of deployments, effort, and outputs","Results from automated acoustic classifiers (e.g., BirdNET, HawkEars)",null,null],["Descriptive Project information from the Methods and Results Summary","Locations specific to the Project, their coordinates, and visibility metadata",null,"Annotations generated by human observers or classifiers","High-level summary of deployments, effort, and outputs",null,"Megadetector","Image set"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>Report<\/th>\n      <th>ARU<\/th>\n      <th>Camera<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"autoWidth":true,"columnDefs":[{"name":"Report","targets":0},{"name":"ARU","targets":1},{"name":"Camera","targets":2}],"order":[],"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
<p><a href="assets/data-discover2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17"><img src="assets/data-discover2.png" class="img-fluid"></a></p>
<p><a href="assets/data-discover3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18"><img src="assets/data-discover3.png" class="img-fluid"></a></p>
</section>
</section>
<section id="analysis-with-wildrtrax" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="analysis-with-wildrtrax">Analysis with wildrtrax</h2>
<p><code>wildrtrax</code> (pronounced wild-r-tracks) is the corresponding R package containing functions to help manage and analyze the corresponding Report data. It also helps to simplify the entire data life cycle with WildTrax by offering tools for data pre-processing (file scanning, renaming), wrangling, and analysis, in order to facilitate seamless data transfer to and from WildTrax. <code>wildrtrax</code> helps users to establish their end-to-end workflows and to help ensure reproducibility in their analyses. <code>wildrtrax</code> functions are called and reflected with GET and POST APIs available throughout the system. All functions begin with <code>wt_</code> to allow ease-of-use of functions. For example users can query the Data Discover APIs using <code>wt_dd_summary()</code> corresponding to the <code>get-dd-map-and-projects</code> and <code>get-dd-long-lat-summary</code> endpoints, and retrieve large-scale datasets, or refine it based on an area of interest.</p>
<div class="cell" data-cap-location="margin">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5" data-cap-location="margin"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">wt_download_report</span>(<span class="at">project_id =</span> <span class="dv">1144</span>,</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>                           <span class="at">sensor_id =</span> <span class="st">"ARU"</span>,</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>                           <span class="at">reports =</span> <span class="fu">c</span>(<span class="st">"main"</span>, <span class="st">"birdnet"</span>), </span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>                           <span class="at">weather_cols =</span> <span class="cn">FALSE</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>eval_ccsp <span class="ot">&lt;-</span> <span class="fu">wt_evaluate_classifier</span>(data,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>                              <span class="at">resolution =</span> <span class="st">"task"</span>,</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>                              <span class="at">remove_species =</span> <span class="cn">TRUE</span>,</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>                              <span class="at">species =</span> <span class="st">"CCSP"</span>,</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>                              <span class="at">thresholds =</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">99</span>))</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co">#Filter the detections to the best threshold</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>threshold_ccsp <span class="ot">&lt;-</span> <span class="fu">wt_classifier_threshold</span>(eval_ccsp)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>detections_ccsp <span class="ot">&lt;-</span> data[[<span class="dv">1</span>]] <span class="sc">|&gt;</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(species_code <span class="sc">==</span> <span class="st">"CCSP"</span>, </span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>         confidence <span class="sc">&gt;</span> threshold_ccsp)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co">#Calculate detections per second and mean confidence in each recording</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>rate_ccsp <span class="ot">&lt;-</span> detections_ccsp <span class="sc">|&gt;</span> </span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(location_id, recording_date_time, recording_length) <span class="sc">|&gt;</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">calls =</span> <span class="fu">n</span>(),</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>            <span class="at">confidence =</span> <span class="fu">mean</span>(confidence),</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>            <span class="at">.groups =</span> <span class="st">"keep"</span>) <span class="sc">|&gt;</span> </span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">|&gt;</span> </span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">rate =</span> calls<span class="sc">/</span>recording_length<span class="sc">*</span><span class="dv">60</span>,</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>  <span class="at">recording_date_time =</span> <span class="fu">as.POSIXct</span>(recording_date_time, <span class="at">format =</span> <span class="st">"%Y-%m-%d %H:%M:%S"</span>),</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>  <span class="at">yday =</span> <span class="fu">as.numeric</span>(<span class="fu">format</span>(recording_date_time, <span class="st">"%j"</span>)),</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>  <span class="at">hour =</span> <span class="fu">as.numeric</span>(<span class="fu">format</span>(recording_date_time, <span class="st">"%H"</span>)))</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a><span class="co">#Filter to the sites with most recordings with detections</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>occupied_ccsp <span class="ot">&lt;-</span> rate_ccsp <span class="sc">|&gt;</span> </span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(location_id) <span class="sc">|&gt;</span> </span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">recordings =</span> <span class="fu">n</span>()) <span class="sc">|&gt;</span> </span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">|&gt;</span> </span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(recordings <span class="sc">&gt;=</span> <span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell page-columns page-full" data-layout-align="center" data-cap-location="margin">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6" data-cap-location="margin"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Plot call rate by day of year</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(occupied_ccsp) <span class="sc">+</span> </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x=</span>yday, <span class="at">y=</span>rate)) <span class="sc">+</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="fu">aes</span>(<span class="at">x=</span>yday, <span class="at">y=</span>rate)) <span class="sc">+</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"Day of year"</span>) <span class="sc">+</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"Rate of Clay-coloured sparrow detections per minute"</span>) <span class="sc">+</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display page-columns page-full">
<div id="fig-ccsp-season" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-cap-location="margin" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-ccsp-season-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="wildtrax-ace-eco_files/figure-html/fig-ccsp-season-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-ccsp-season-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Clay-coloured detection rates across the season in the ABMI Ecosystem Health 2022 dataset.
</figcaption>
</figure>
</div>
</div>
</div>
<p>BirdNET and HawkEars output can be used to automatically classify species in Tasks. Classifier outputs are downloaded via <code>wt_download_report()</code> and combined with the main project report for evaluation. Classifier performance is quantified using precision, recall, and F-score across score thresholds (<code>wt_evaluate_classifier()</code>), and thresholds can be selected to maximize F-score (<code>wt_classifier_threshold()</code>). BirdNET and HawkEars can also identify species missed by human listeners, enhancing species richness estimates (<code>wt_additional_species()</code>). For behavioural analyses, individual call rates can be quantified using species-specific evaluation (<code>wt_evaluate_classifier()</code> for selected species) and filtering detections by optimal thresholds. Call rate trends can then be analyzed temporally or spatially, as demonstrated for Clay-coloured Sparrow (<em>Spizella pallida</em>) in <a href="#fig-ccsp-season" class="quarto-xref">Figure&nbsp;1</a> modelling detection rates across the season (see also https://abbiodiversity.github.io/wildrtrax/index.html).</p>
</section>
</section>
<section id="results" class="level1 page-columns page-full">
<h1>Results</h1>
<p>With the release of acoustic sensors in 2018 and camera sensors in 2020 within the WildTrax platform, <a href="#fig-data-accum" class="quarto-xref">Figure&nbsp;3</a> illustrates the cumulative growth of remote camera images and acoustic recordings managed through the ABMI and WildTrax frameworks. Since the launch of WildTrax 1.0 in 2018, contributions from ABMI internal programs and partner organizations have resulted in over 150 million images and 2.5 million acoustic recordings by 2025, reflecting substantial platform adoption. The rate of data accumulation has increased exponentially on an annual basis, indicating not only the expanding use of WildTrax but also the growing engagement of contributing organizations.</p>
<div class="cell page-columns page-full" data-layout-align="center" data-cap-location="margin">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7" data-cap-location="margin"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>species_acoustic_tags <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"assets/acoustictags.csv"</span>) <span class="sc">|&gt;</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(species_code <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">"WTSP"</span>,<span class="st">"SWTH"</span>,<span class="st">"YEWA"</span>,<span class="st">"REVI"</span>,<span class="st">"OSFL"</span>,<span class="st">"CONI"</span>,<span class="st">"RUGR"</span>,<span class="st">"FRGU"</span>,<span class="st">"SPSA"</span>,<span class="st">"RTHA"</span>)) <span class="sc">|&gt;</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(species_code, date_added_on) <span class="sc">|&gt;</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">count =</span> <span class="fu">sum</span>(tag_count), <span class="at">.groups =</span> <span class="st">"drop_last"</span>) <span class="sc">|&gt;</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(species_code, date_added_on) <span class="sc">|&gt;</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">cumulative_count =</span> <span class="fu">cumsum</span>(count)) <span class="sc">|&gt;</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>()</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(species_acoustic_tags, <span class="fu">aes</span>(<span class="at">x=</span>date_added_on, <span class="at">y=</span>cumulative_count, <span class="at">colour=</span>species_code)) <span class="sc">+</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">fill =</span> <span class="st">"#FFFFEE"</span>) <span class="sc">+</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_colour_viridis_d</span>(<span class="at">name =</span> <span class="st">"Species"</span>) <span class="sc">+</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_log10</span>(<span class="at">labels =</span> <span class="fu">trans_format</span>(<span class="st">"log10"</span>, <span class="fu">math_format</span>(<span class="dv">10</span><span class="sc">^</span>.x))) <span class="sc">+</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"Log cumulative count of acoustic species tags over time"</span>) <span class="sc">+</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"Date"</span>) <span class="sc">+</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="st">"Species"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display page-columns page-full">
<div id="fig-acoustic-spp" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-cap-location="margin" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-acoustic-spp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="wildtrax-ace-eco_files/figure-html/fig-acoustic-spp-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-acoustic-spp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: All publicly accessible cumulative species detections in WildTrax for ten avian species across multiple taxonomic groups
</figcaption>
</figure>
</div>
</div>
</div>
<p>Projects within the platform cover spatial scales ranging from local sites to provincial-level networks. A growing proportion of these projects are publicly accessible, demonstrating the scalability, interoperability, and broad applicability of the system for diverse monitoring initiatives. Currently, public datasets, primarily derived from point count sensors deployed through BAM collaborations, constitute approximately 10% of total Project holdings, covering an area of [insert area] km² and encompassing 263 projects (205 ARU and point count, 58 camera) across 31 organizations. Ecologically, the dataset enables high-resolution monitoring of species occurrence and abundance. For example, the total number of White-throated Sparrow (<em>Zonotrichia albicollis</em>) detections now exceeds 5485 individual locations, providing an already large dataset for long-term population and distribution analyses. These accumulating records illustrate the capacity of WildTrax to support large-scale ecological research, facilitate data sharing, and generate comprehensive biodiversity insights across multiple taxa and monitoring modalities.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-data-accum" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-data-accum-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="wildtrax-ace-eco_files/figure-html/fig-data-accum-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:100.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-data-accum-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Accumulation of media since WildTrax (2018) and ABMI’s (2014) inception of environmental sensor monitoring programs
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply an area of interest. Define a polygon or use a bbox from sf::st_bbox</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>my_aoi <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(<span class="sc">-</span><span class="fl">113.96068</span>, <span class="fl">56.23817</span>),</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(<span class="sc">-</span><span class="fl">117.06285</span>, <span class="fl">54.87577</span>),</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(<span class="sc">-</span><span class="fl">112.88035</span>, <span class="fl">54.90431</span>),</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">c</span>(<span class="sc">-</span><span class="fl">113.96068</span>, <span class="fl">56.23817</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>ab_cd <span class="ot">&lt;-</span> sf<span class="sc">::</span><span class="fu">read_sf</span>(<span class="st">".../Alberta_Census_Boundaries_SHP/Data/AB_CD_2021.shp"</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>abbox <span class="ot">&lt;-</span> ab_cd <span class="sc">|&gt;</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>  sf<span class="sc">::</span><span class="fu">st_transform</span>(<span class="at">crs =</span> <span class="dv">4326</span>) <span class="sc">|&gt;</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>  sf<span class="sc">::</span><span class="fu">st_bbox</span>()</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>my_data <span class="ot">&lt;-</span> wildrtrax<span class="sc">::</span><span class="fu">wt_dd_summary</span>(<span class="at">sensor =</span> <span class="st">'ARU'</span>, <span class="at">species =</span> <span class="st">'White-throated Sparrow'</span>, <span class="at">boundary =</span> abbox)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell page-columns page-full" data-layout-align="center" data-cap-location="margin">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9" data-cap-location="margin"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_sf</span>(<span class="at">data =</span> ab_cd, <span class="at">fill =</span> <span class="st">"white"</span>, <span class="at">color =</span> <span class="st">"black"</span>) <span class="sc">+</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> my_data[[<span class="dv">2</span>]], <span class="fu">aes</span>(<span class="at">x =</span> longitude, <span class="at">y =</span> latitude, <span class="at">size =</span> count), <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">alpha =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_sf</span>(<span class="at">crs =</span> <span class="fu">st_crs</span>(<span class="dv">4326</span>)) <span class="sc">+</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Longitude"</span>, <span class="at">y =</span> <span class="st">"Latitude"</span>,</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">caption =</span> <span class="st">"Source: WildTrax Data Discover"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display page-columns page-full">
<div id="fig-dd-wtsp" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-cap-location="margin" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-dd-wtsp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="wildtrax-ace-eco_files/figure-html/fig-dd-wtsp-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-dd-wtsp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: All publicly accessible White-throated Sparrow (Zonotrichia albicollis) detections in Alberta in WildTrax’s Data Discover portal queried through the wildrtrax package function wt_dd_summary
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="discussion-and-future-prospects" class="level1">
<h1>Discussion and future prospects</h1>
<p>The expansion of environmental sensor networks for avian monitoring has fundamentally altered how ecological data can be collected, managed, and interpreted. These trends in software uptake emphasize the importance of continually upgrading and handling data management strategies, optimized storage solutions, and creating efficient analytical tools to ensure continued accessibility and usability of increasing big data. High-resolution, continuous recordings from ARUs and remote cameras generate unprecedented volumes of temporal and spatial data, yet the potential of these datasets is only realized through structured, scalable, and accessible management frameworks. Platforms such as WildTrax exemplify this need, offering centralized solutions that enable integration across sensor types, standardization of metadata, and harmonization of multi-source observations. Ongoing considerations in deploying these systems is the balance between data volume and interpretability.</p>
<p>While sensors and artificial intelligence results can produce vast archives of audio and visual media, ensuring data quality and reducing noise or misclassification remains under ongoing management. Analytical pipelines that incorporate deep learning, combined with human verification, provide a mechanism to efficiently extract meaningful ecological signals while maintaining transparency and user control, shepherding futher human-computer collaboration. The potential of these platforms extends beyond single or multi-taxa monitoring; by linking fine-scale temporal datasets with spatially distributed sensors, researchers can begin to explore community-level patterns, seasonal dynamics, and multi-species interactions at scales that were previously unattainable. Furthermore, standardized, open-access repositories facilitate reproducibility and comparative studies, allowing new ecological questions to be addressed without the need to recollect data. This is particularly relevant for studies of ecosystem health and biodiversity trends, where longitudinal and multi-site datasets provide the statistical power required to detect subtle changes.</p>
<p>The adoption of integrated sensor platforms also raises considerations for infrastructure, cost, and workflow design. Effective long-term monitoring relies not only on hardware reliability and data storage solutions, but also on user-friendly interfaces that allow project administrators and contributors to manage complex datasets efficiently. By supporting automated workflows alongside human oversight, platforms can reduce observer bias, improve detection accuracy, and provide verifiable, permanent records for ecological studies. Overall, while technological and computational advances have enabled large-scale environmental monitoring, the realization of these benefits is contingent upon robust data management systems. The development and refinement of platforms like WildTrax illustrate how socio-technical frameworks can mediate between raw sensor outputs and actionable ecological insights, creating opportunities for broader collaboration, reproducibility, and adaptive conservation practices across multiple spatial and temporal scales.</p>
</section>
<section id="glossary" class="level1">
<h1>Glossary</h1>
<ul>
<li><strong>Organization:</strong> A collection of users who manage <strong>Locations</strong>, <strong>Equipment</strong>, <strong>Deployments</strong>, <strong>Recordings</strong>, and <strong>Image Sets</strong>.<br>
</li>
<li><strong>Visits:</strong> Occasions when a user goes to a <strong>Location</strong> to collect data.<br>
</li>
<li><strong>Deployments:</strong> The act of placing a piece of <strong>Equipment</strong> at a <strong>Location</strong> during a <strong>Visit</strong>.<br>
</li>
<li><strong>Equipment:</strong> Environmental sensor devices, such as <strong>ARUs</strong>, remote camera traps, SD cards, or microphones.</li>
<li><strong>Project:</strong> A collaborative effort by a group of users who design studies and process <strong>Tasks</strong> to answer scientific questions.<br>
</li>
<li><strong>Location:</strong> A physical, geographic site where environmental sensors (such as <strong>ARUs</strong> or cameras) are placed.<br>
</li>
<li><strong>ARU (Autonomous Recording Unit):</strong> A self-contained device that records environmental audio for research purposes.<br>
</li>
<li><strong>Tasks:</strong> Unique combinations of a user, processing method, and media used to answer a scientific question.</li>
<li><strong>Tags:</strong> Enclosed portions of a <strong>Recording</strong> or <strong>Image</strong> that contain a species detection.</li>
</ul>
</section>
<section id="additional-information-and-declarations" class="level1">
<h1>Additional Information and Declarations</h1>
<section id="competing-interests" class="level2">
<h2 class="anchored" data-anchor-id="competing-interests">Competing Interests</h2>
<p>The authors declare no competing interests.</p>
</section>
<section id="author-contributions" class="level2">
<h2 class="anchored" data-anchor-id="author-contributions">Author Contributions</h2>
<p>All authors contributed to manuscript edits and revisions.</p>
<p>Alexander G. MacPhail led the manuscript preparation and design, as well as user support and engagement for the acoustic sensor.</p>
<p>Corrina Copp led platform development, project and data management, and oversaw the initial design and implementation of the camera sensor.</p>
<p>Erin M. Bayne made formative and ongoing contributions to the conceptual development, study design, and overall scientific guidance of the project.</p>
<p>Michael Packer served as lead developer, overseeing server architecture, back-end development, system scalability, user support, and ensuring optimized data processing workflows.</p>
<p>Chad Klassen designed the user interface, provided front-end development, user support, and led user experience design to optimize accessibility and usability across devices.</p>
<p>Joan Fang helped with the initial concept and development.</p>
<p>Hedwig E. Lankau contributed to the development of the Bioacoustic Information System, which provided the foundational framework for the design of the acoustic processing system.</p>
<p>Monica Kohler provided support during initial funding and conceptual stages.</p>
<p>Tara Narwani provided support during initial funding and conceptual stages.</p>
<p>Steve L. Van Wilgenburg provided immeasurable feedback through the creation and use of the system at scale.</p>
<p>Elly C. Knight provided data standardization, integration and analytical support especially with acoustic classifiers.</p>
<p>Kevin G. Kelly provided design implementation and testing especially with acoustic classifiers, as well as user support.</p>
<p>Charles M. Francis provided support, inspiration, and advocacy, which helped secure the funding that made this WildTrax possible. His guidance was instrumental in initiating WildTrax and in shaping earlier work with Avichorus, which served as an inspiration for the platform.</p>
</section>
</section>
<section id="acknowledgements" class="level1">
<h1>Acknowledgements</h1>
<p>WildTrax was conceived in Edmonton, ᐊᒥᐢᑿᒌᐚᐢᑲᐦᐃᑲᐣ Amiskwaciwâskahikan, located within Treaty 6 Territory and within the Métis homelands and Métis Nation of Alberta Region 4. We acknowledge this land as the traditional territories of many First Nations such as the Nehiyaw (Cree), Denesuliné (Dene), Nakota Sioux (Stoney), Anishinaabe (Saulteaux) and Niitsitapi (Blackfoot). This project was supported by funding from Environment and Climate Change Canada, Alberta Environment and Parks, the Oil Sands Monitoring Program, and Canada’s Oil Sands Innovation Alliance. We also extend our gratitude to the following organizations for their partnership and support: U of A Sound Studies Institute, Joint Canada-Alberta Implementation Plan for Oil Sands Monitoring, InnoTech Alberta, University of Alberta, NSERC (Natural Sciences and Engineering Research Council of Canada), PTAC, Devon Energy, ConocoPhillips, Cenovus Energy, Nexen, Imperial Oil, Shell, Suncor Energy, Alberta Pacific Forest Industries Inc., Canadian Natural, Alberta Conservation Association, Parks Canada, University of Alberta, Government of Saskatchewan, and Ɂehdzo Got’ı̨nę Gots’ę́ Nákedı Sahtú Renewable Resources Board. We acknowledge that the data used in WildTrax were collected on lands that are, and have always been, the traditional territories of Indigenous peoples. We honour their enduring connection to these lands, waters, and communities, and recognize their rights, cultures, and contributions. We thank the WildTrax community for their ongoing contributions, which strengthen the platform and advance big data approaches to avian conservation.</p>

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-ahumada2020wildlife" class="csl-entry" role="listitem">
Ahumada, Jorge A, Eric Fegraus, Tanya Birch, Nicole Flores, Roland Kays, Timothy G O’Brien, Jonathan Palmer, et al. 2020. <span>“Wildlife Insights: A Platform to Maximize the Potential of Camera Trap and Other Passive Sensor Wildlife Data for the Planet.”</span> <em>Environmental Conservation</em> 47 (1): 1–6.
</div>
<div id="ref-aide2013real" class="csl-entry" role="listitem">
Aide, T Mitchell, Carlos Corrada-Bravo, Marconi Campos-Cerqueira, Carlos Milan, Giovany Vega, and Rafael Alvarez. 2013. <span>“Real-Time Bioacoustics Monitoring and Automated Species Identification.”</span> <em>PeerJ</em> 1: e103.
</div>
<div id="ref-beery2023megadetector" class="csl-entry" role="listitem">
Beery, Sara. 2023. <span>“The MegaDetector: Large-Scale Deployment of Computer Vision for Conservation and Biodiversity Monitoring.”</span> <em>California Institute of Technology, Pasadena, CA, USA</em>.
</div>
<div id="ref-bibby1999making" class="csl-entry" role="listitem">
Bibby, Colin J. 1999. <span>“Making the Most of Birds as Environmental Indicators.”</span> <em>Ostrich</em> 70 (1): 81–88.
</div>
<div id="ref-binley2023minimizing" class="csl-entry" role="listitem">
Binley, Allison D, Brandon PM Edwards, Gabriel Dansereau, Elly C Knight, and Iman Momeni-Dehaghi. 2023. <span>“Minimizing Data Waste.”</span> <em>Bulletin of the Ecological Society of America</em> 104 (2): 1–11.
</div>
<div id="ref-bolton2007remote" class="csl-entry" role="listitem">
Bolton, Mark, Nigel Butcher, Fiona Sharpe, Danaë Stevens, and Gareth Fisher. 2007. <span>“Remote Monitoring of Nests Using Digital Camera Technology.”</span> <em>Journal of Field Ornithology</em> 78 (2): 213–20.
</div>
<div id="ref-buxton2021key" class="csl-entry" role="listitem">
Buxton, Rachel T, Joseph R Bennett, Andrea J Reid, Charles Shulman, Steven J Cooke, Charles M Francis, Elizabeth A Nyboer, et al. 2021. <span>“Key Information Needs to Move from Knowledge to Action for Biodiversity Conservation in Canada.”</span> <em>Biological Conservation</em> 256: 108983.
</div>
<div id="ref-buxton2018pairing" class="csl-entry" role="listitem">
Buxton, Rachel T, Patrick E Lendrum, Kevin R Crooks, and George Wittemyer. 2018. <span>“Pairing Camera Traps and Acoustic Recorders to Monitor the Ecological Impact of Human Disturbance.”</span> <em>Global Ecology and Conservation</em> 16: e00493.
</div>
<div id="ref-canterbury2000bird" class="csl-entry" role="listitem">
Canterbury, Grant E, Thomas E Martin, Daniel R Petit, Lisa J Petit, and David F Bradford. 2000. <span>“Bird Communities and Habitat as Ecological Indicators of Forest Condition in Regional Monitoring.”</span> <em>Conservation Biology</em> 14 (2): 544–58.
</div>
<div id="ref-douglas2003postgresql" class="csl-entry" role="listitem">
Douglas, Korry, and Susan Douglas. 2003. <em>PostgreSQL: A Comprehensive Guide to Building, Programming, and Administering PostgresSQL Databases</em>. SAMS publishing.
</div>
<div id="ref-farley2018situating" class="csl-entry" role="listitem">
Farley, Scott S, Andria Dawson, Simon J Goring, and John W Williams. 2018. <span>“Situating Ecology as a Big-Data Science: Current Advances, Challenges, and Solutions.”</span> <em>BioScience</em> 68 (8): 563–76.
</div>
<div id="ref-fleishman2005using" class="csl-entry" role="listitem">
Fleishman, Erica, James R Thomson, Ralph Mac Nally, Dennis D Murphy, and John P Fay. 2005. <span>“Using Indicator Species to Predict Species Richness of Multiple Taxonomic Groups.”</span> <em>Conservation Biology</em> 19 (4): 1125–37.
</div>
<div id="ref-fox2017generating" class="csl-entry" role="listitem">
Fox, Helen E, Megan D Barnes, Gabby N Ahmadia, Grace Kao, Louise Glew, Kelly Haisfield, Nur Ismu Hidayat, et al. 2017. <span>“Generating Actionable Data for Evidence-Based Conservation: The Global Center of Marine Biodiversity as a Case Study.”</span> <em>Biological Conservation</em> 210: 299–309.
</div>
<div id="ref-fraixedas2020state" class="csl-entry" role="listitem">
Fraixedas, Sara, Andreas Lindén, Markus Piha, Mar Cabeza, Richard Gregory, and Aleksi Lehikoinen. 2020. <span>“A State-of-the-Art Review on Birds as Indicators of Biodiversity: Advances, Challenges, and Future Directions.”</span> <em>Ecological Indicators</em> 118: 106728.
</div>
<div id="ref-furness2013birds" class="csl-entry" role="listitem">
Furness, Robert W, and Jeremy JD Greenwood. 2013. <em>Birds as Monitors of Environmental Change</em>. Springer Science &amp; Business Media.
</div>
<div id="ref-garland2020acoustic" class="csl-entry" role="listitem">
Garland, Laura, Andrew Crosby, Richard Hedley, Stan Boutin, and Erin Bayne. 2020. <span>“Acoustic Vs. Photographic Monitoring of Gray Wolves (Canis Lupus): A Methodological Comparison of Two Passive Monitoring Techniques.”</span> <em>Canadian Journal of Zoology</em> 98 (3): 219–28.
</div>
<div id="ref-greenberg2012timelapse" class="csl-entry" role="listitem">
Greenberg, Saul, and Theresa Godin. 2012. <span>“Timelapse Image Analysis Manual.”</span>
</div>
<div id="ref-gregory2003using" class="csl-entry" role="listitem">
Gregory, Richard D, David Noble, Rob Field, John Marchant, M Raven, and DW Gibbons. 2003. <span>“Using Birds as Indicators of Biodiversity.”</span> <em>Ornis Hungarica</em> 12 (13): 11–24.
</div>
<div id="ref-hallgren2016biodiversity" class="csl-entry" role="listitem">
Hallgren, Willow, Linda Beaumont, Andrew Bowness, Lynda Chambers, Erin Graham, Hamish Holewa, Shawn Laffan, et al. 2016. <span>“The Biodiversity and Climate Change Virtual Laboratory: Where Ecology Meets Big Data.”</span> <em>Environmental Modelling &amp; Software</em> 76: 182–86.
</div>
<div id="ref-hampton2013big" class="csl-entry" role="listitem">
Hampton, Stephanie E, Carly A Strasser, Joshua J Tewksbury, Wendy K Gram, Amber E Budden, Archer L Batcheller, Clifford S Duke, and John H Porter. 2013. <span>“Big Data and the Future of Ecology.”</span> <em>Frontiers in Ecology and the Environment</em> 11 (3): 156–62.
</div>
<div id="ref-hobson2002acoustic" class="csl-entry" role="listitem">
Hobson, Keith A, Robert S Rempel, Hamilton Greenwood, Brian Turnbull, and Steven L Van Wilgenburg. 2002. <span>“Acoustic Surveys of Birds Using Electronic Recordings: New Potential from an Omnidirectional Microphone System.”</span> <em>Wildlife Society Bulletin</em>, 709–20.
</div>
<div id="ref-jetz2012global" class="csl-entry" role="listitem">
Jetz, Walter, Gavin H Thomas, Jeffery B Joy, Klaas Hartmann, and Arne O Mooers. 2012. <span>“The Global Diversity of Birds in Space and Time.”</span> <em>Nature</em> 491 (7424): 444–48.
</div>
<div id="ref-jiguet2007climate" class="csl-entry" role="listitem">
Jiguet, Frédéric, ANNE-SOPHIE GADOT, Romain Julliard, Stuart E Newson, and Denis Couvet. 2007. <span>“Climate Envelope, Life History Traits and the Resilience of Birds Facing Global Change.”</span> <em>Global Change Biology</em> 13 (8): 1672–84.
</div>
<div id="ref-kartez2008information" class="csl-entry" role="listitem">
Kartez, Jack D, and Molly P Casto. 2008. <span>“Information into Action: Biodiversity Data Outreach and Municipal Land Conservation.”</span> <em>Journal of the American Planning Association</em> 74 (4): 467–80.
</div>
<div id="ref-kim2021ecobank" class="csl-entry" role="listitem">
Kim, Hyun Woo, Sungsoo Yoon, Mokyoung Kim, Manseok Shin, Heenam Yoon, and Kidong Kim. 2021. <span>“EcoBank: A Flexible Database Platform for Sharing Ecological Data.”</span> <em>Biodiversity Data Journal</em> 9: e61866.
</div>
<div id="ref-kush2020fair" class="csl-entry" role="listitem">
Kush, Rebecca Daniels, D Warzel, Maura A Kush, Alexander Sherman, Eileen A Navarro, R Fitzmartin, Frank Pétavy, et al. 2020. <span>“FAIR Data Sharing: The Roles of Common Data Elements and Harmonization.”</span> <em>Journal of Biomedical Informatics</em> 107: 103421.
</div>
<div id="ref-lepage2014avibase" class="csl-entry" role="listitem">
Lepage, Denis, Gaurav Vaidya, and Robert Guralnick. 2014. <span>“Avibase–a Database System for Managing and Organizing Taxonomic Concepts.”</span> <em>ZooKeys</em>, no. 420: 117.
</div>
<div id="ref-macphail2024audio" class="csl-entry" role="listitem">
MacPhail, Alexander G, Daniel A Yip, Elly C Knight, Richard Hedley, Michelle Knaggs, Julia Shonfield, Emily Upham-Mills, and Erin M Bayne. 2024. <span>“Audio Data Compression Affects Acoustic Indices and Reduces Detections of Birds by Human Listening and Automated Recognisers.”</span> <em>Bioacoustics</em> 33 (1): 74–90.
</div>
<div id="ref-mekonen2017birds" class="csl-entry" role="listitem">
Mekonen, Sefi. 2017. <span>“Birds as Biodiversity and Environmental Indicator.”</span> <em>Indicator</em> 7 (21).
</div>
<div id="ref-michel2020metrics" class="csl-entry" role="listitem">
Michel, Nicole L, Curtis Burkhalter, Chad B Wilsey, Matt Holloran, Alison Holloran, and Gary M Langham. 2020. <span>“Metrics for Conservation Success: Using the <span>‘Bird-Friendliness Index’</span> to Evaluate Grassland and Aridland Bird Community Resilience Across the Northern Great Plains Ecosystem.”</span> <em>Diversity and Distributions</em> 26 (12): 1687–1702.
</div>
<div id="ref-morante2017appraisal" class="csl-entry" role="listitem">
Morante-Filho, José Carlos, and Deborah Faria. 2017. <span>“An Appraisal of Bird-Mediated Ecological Functions in a Changing World.”</span> <em>Tropical Conservation Science</em> 10: 1940082917703339.
</div>
<div id="ref-nathan2022big" class="csl-entry" role="listitem">
Nathan, Ran, Christopher T Monk, Robert Arlinghaus, Timo Adam, Josep Alós, Michael Assaf, Henrik Baktoft, et al. 2022. <span>“Big-Data Approaches Lead to an Increased Understanding of the Ecology of Animal Movement.”</span> <em>Science</em> 375 (6582): eabg1780.
</div>
<div id="ref-newman2007aquatic" class="csl-entry" role="listitem">
Newman, Scott H, Aleksei Chmura, Kathy Converse, A Marm Kilpatrick, Nikkita Patel, Emily Lammers, and Peter Daszak. 2007. <span>“Aquatic Bird Disease and Mortality as an Indicator of Changing Ecosystem Health.”</span> <em>Marine Ecology Progress Series</em> 352: 299–309.
</div>
<div id="ref-niemi1997critical" class="csl-entry" role="listitem">
Niemi, Gerald J, Joann M Hanowski, Ann R Lima, Tom Nicholls, and Norm Weiland. 1997. <span>“A Critical Analysis on the Use of Indicator Species in Management.”</span> <em>The Journal of Wildlife Management</em>, 1240–52.
</div>
<div id="ref-o2008picture" class="csl-entry" role="listitem">
O’Brien, Timothy G, and Margaret F Kinnaird. 2008. <span>“A Picture Is Worth a Thousand Words: The Application of Camera Trapping to the Study of Birds.”</span> <em>Bird Conservation International</em> 18 (S1): S144–62.
</div>
<div id="ref-orme2006global" class="csl-entry" role="listitem">
Orme, C David L, Richard G Davies, Valerie A Olson, Gavin H Thomas, Tzung-Su Ding, Pamela C Rasmussen, Robert S Ridgely, et al. 2006. <span>“Global Patterns of Geographic Range Size in Birds.”</span> <em>PLoS Biology</em> 4 (7): e208.
</div>
<div id="ref-perkel2019ways" class="csl-entry" role="listitem">
Perkel, Jeffrey M. 2019. <span>“Ways to Avoid a Data-Storage Disaster.”</span> <em>Nature</em> 568 (7750): 131–32.
</div>
<div id="ref-peters2014harnessing" class="csl-entry" role="listitem">
Peters, Debra PC, Kris M Havstad, Judy Cushing, Craig Tweedie, Olac Fuentes, and Natalia Villanueva-Rosales. 2014. <span>“Harnessing the Power of Big Data: Infusing the Scientific Method with Machine Learning to Transform Ecology.”</span> <em>Ecosphere</em> 5 (6): 1–15.
</div>
<div id="ref-pollet2025technological" class="csl-entry" role="listitem">
Pollet, Ingrid L, Alexa Arnyek, Julia Ellen Baak, Rikki Clark, Jacob Comeau-Ouellette, Asha C Grewal, Sarah E Gutowsky, et al. 2025. <span>“Technological Advancements: A Global Review of the Use of Camera Technology in Wildlife Research.”</span> <em>Environmental Reviews</em>, no. ja.
</div>
<div id="ref-randler2018distance" class="csl-entry" role="listitem">
Randler, Christoph, and Nadine Kalb. 2018. <span>“Distance and Size Matters: A Comparison of Six Wildlife Camera Traps and Their Usefulness for Wild Birds.”</span> <em>Ecology and Evolution</em> 8 (14): 7151–63.
</div>
<div id="ref-reif2013long" class="csl-entry" role="listitem">
Reif, Jiřı́. 2013. <span>“Long-Term Trends in Bird Populations: A Review of Patterns and Potential Drivers in North America and Europe.”</span> <em>Acta Ornithologica</em> 48 (1): 1–16.
</div>
<div id="ref-sekercioglu2012bird" class="csl-entry" role="listitem">
Sekercioglu, Cagan H. 2012. <span>“Bird Functional Diversity and Ecosystem Services in Tropical Forests, Agroforests and Agricultural Areas.”</span> <em>Journal of Ornithology</em> 153 (Suppl 1): 153–61.
</div>
<div id="ref-shin2015ecological" class="csl-entry" role="listitem">
Shin, Dong-Hee, and Min Jae Choi. 2015. <span>“Ecological Views of Big Data: Perspectives and Issues.”</span> <em>Telematics and Informatics</em> 32 (2): 311–20.
</div>
<div id="ref-shonfield2017autonomous" class="csl-entry" role="listitem">
Shonfield, Julia, and Erin M Bayne. 2017. <span>“Autonomous Recording Units in Avian Ecological Research: Current Use and Future Applications.”</span> <em>Avian Conservation &amp; Ecology</em> 12 (1).
</div>
<div id="ref-sitters2016bird" class="csl-entry" role="listitem">
Sitters, Holly, Julian Di Stefano, Fiona Christie, Matthew Swan, and Alan York. 2016. <span>“Bird Functional Diversity Decreases with Time Since Disturbance: Does Patchy Prescribed Fire Enhance Ecosystem Function?”</span> <em>Ecological Applications</em> 26 (1): 115–27.
</div>
<div id="ref-smits2013avian" class="csl-entry" role="listitem">
Smits, Judit EG, and Kimberly J Fernie. 2013. <span>“Avian Wildlife as Sentinels of Ecosystem Health.”</span> <em>Comparative Immunology, Microbiology and Infectious Diseases</em> 36 (3): 333–42.
</div>
<div id="ref-solymos2013calibrating" class="csl-entry" role="listitem">
Sólymos, Péter, Steven M Matsuoka, Erin M Bayne, Subhash R Lele, Patricia Fontaine, Steve G Cumming, Diana Stralberg, Fiona KA Schmiegelow, and Samantha J Song. 2013. <span>“Calibrating Indices of Avian Density from Non-Standardized Survey Data: Making the Most of a Messy Situation.”</span> <em>Methods in Ecology and Evolution</em> 4 (11): 1047–58.
</div>
<div id="ref-stanton2016flexible" class="csl-entry" role="listitem">
Stanton, Jessica C, Brice X Semmens, Patrick C McKann, Tom Will, and Wayne E Thogmartin. 2016. <span>“Flexible Risk Metrics for Identifying and Monitoring Conservation-Priority Species.”</span> <em>Ecological Indicators</em> 61: 683–92.
</div>
<div id="ref-stephenson2020inventory" class="csl-entry" role="listitem">
Stephenson, PJ, and Carrie Stengel. 2020. <span>“An Inventory of Biodiversity Data Sources for Conservation Monitoring.”</span> <em>PLoS One</em> 15 (12): e0242923.
</div>
<div id="ref-temple1989bird" class="csl-entry" role="listitem">
Temple, Stanley A, and John A Wiens. 1989. <span>“Bird Populations and Environmental Changes: Can Birds Be Bio-Indicators?”</span> <em>American Birds</em> 43 (2): 14.
</div>
<div id="ref-yip2017experimentally" class="csl-entry" role="listitem">
Yip, Daniel A, Lionel Leston, Erin M Bayne, Péter Sólymos, and Alison Grover. 2017. <span>“Experimentally Derived Detection Distances from Audio Recordings and Human Observers Enable Integrated Analysis of Point Count Data.”</span> <em>Avian Conservation and Ecology</em> 12 (1): 11.
</div>
<div id="ref-zakaria2005comparison" class="csl-entry" role="listitem">
Zakaria, Mohamed, Puan Chong Leong, and Muhammad Ezhar Yusuf. 2005. <span>“Comparison of Species Composition in Three Forest Types: Towards Using Bird as Indicator of Forest Ecosystem Health.”</span> <em>Journal of Biological Sciences</em> 5 (6): 734–37.
</div>
<div id="ref-zhang2009efficiently" class="csl-entry" role="listitem">
Zhang, Jianting, Michael Gertz, and Le Gruenwald. 2009. <span>“Efficiently Managing Large-Scale Raster Species Distribution Data in PostgreSQL.”</span> In <em>Proceedings of the 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems</em>, 316–25.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"descPosition":"bottom","selector":".lightbox","openEffect":"zoom","closeEffect":"zoom","loop":false});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>